\chapter{Gatling - Testing Framework}

In diesem Kapitel soll auf das Testing Framework \textit{Gatling.io} eingegangen werden. Im Besonderen wird analysiert, was Performance-Tests sind und welche Vorteile diese für den Entwickler bzw. die Softwarearchitektur mit sich bringen.
\newline
Nachdem mögliche Einsatzgebiete von Performance-Tests analysiert wurden, wird auf die \ac{DSL} von Gatling.io eingegangen. Es werden hier die wichtigsten Befehle erläutert.

%https://gatling.io/

\section{Performance-Tests}

Performance Tests sind für jede größere Anwendung von essentieller Bedeutung. Durch diese Art der Tests können sogenannte \textit{Bottlenecks} der Anwendung ausfindig gemacht und beseitigt werden. Unter den \textit{Bottlenecks} versteht man einzelne Komponenten der programmierten Anwendung, die den gesamten Worfklow ausbremsen. Als Beispiel könnte folgende Situation dienen:
\begin{itemize}
    \item Eine aufwendigere Berechnung wurde in parallele Tasks ausgelagert
    \item Jeder der Tasks muss eine Datenbankabfrage durchführen um die aktuellsten Daten zur Verfügung zu haben
    \item Durch diese langwierige Abfrage verzögert sich die gesamte Berechnung
\end{itemize}
Als Bottleneck ist hier die Abfrage zur Datenbank aufzuführen, diese sollte in einen vorgelagerten Prozess abgespalten werden. Auch wenn dieses konstruierte Beispiel sehr einfach und überzogen dargestellt ist, gibt es in echten Anwendungen auch Problembereiche, die vom Entwickler nicht als solche identifiziert werden konnten. Beispielsweise könnte eine Abfrage an eine Schnittstelle unter normalen Bedingungen sofort erledigt sein, unter Last allerdings ändert sich dessen Verhalten. Ohne einen Test, der eine konstruierte Last bzw. Auslastung auf der Anwendung erzeugt, wird diese Art von Fehler nicht entdeckt. Der Administrator kann so im Vornherein die Infrastruktur anpassen, bevor die Anwendung im produktiven Umfeld ist.
 
\subsection{Einsatzgebiete}

\begin{itemize}
    \item Vorhersage von Bottlenecks in Anwendung:
    
    Durch einen Test, der eine simulierte Auslastung auf der Anwendung erzeugt, können einzelne Problembereiche der Anwendung aufgezeigt werden.
    \item Skalierung der Anwendung:

    Durch Performance-Tests kann eine beliebige Anzahl an Usern auf der Anwendung simuliert werden. Es könnte beispielsweise die Skalierung der Anwendung erprobt werden. Hier wird nicht nur die Softwarearchitektur der Anwendung getestet, sondern auch die IT- Infrastruktur. Durch die genannten Tests könnte beispielsweise aufkommen, dass ein Load-Balancer für eine erwartete Last von 1 Million Benutzern nicht ausreichend ist.
    \item Benutzer Bedienung verbessern -- Latenz Zeiten verringern = schnelleres Feedback für den User
    \item langfristig Verbesserung / Optimierung der Software Architektur  --> wie gut skaliert die Anwendung?

\end{itemize}


\section{DSL}

%https://gatling.io/docs/current/cheat-sheet/

\subsection{Wichtige Befehle}

\begin{table}[]
\centering
\caption{Cheat-Sheet DSL Gatling.io}
\label{table_cheatSheetDSL}
\begin{tabular}{|
>{\columncolor[HTML]{FCFF2F}}l |
>{\columncolor[HTML]{67FD9A}}l |}
\hline
\cellcolor[HTML]{C0C0C0}Befehl & \cellcolor[HTML]{C0C0C0}Funktionsweise \\ \hline
{\color[HTML]{333333} Erster Test} & Testfunktion \\ \hline
Test & Testfunktion \\ \hline
\end{tabular}

\end{table}
%Tabelle mit wichtigsten Befehlen zeigen


\section{Metriken}

Welche Metriken sind verfügbar ?
Wie werden diese gesammelt?
Wie werden diese interpretiert?




\section{Continous Integration}

\begin{figure}
	{\caption{Continous Integration Prozess}
		\label{fig:continousIntegration}}
	{\includegraphics[scale=0.8]{\figdir/continuous-testing}}
\end{figure}


Der Einsatz von Buildservern in einer Continous Integration Umgebung gehört mittlerweile zu den Standard Werkzeugen eines Entwicklerteams. Aber was ist Continous Integration und wie können Lasttests in diesen Prozess eingebunden werden?

\begin{figure}
	{\caption{Ergebnis der Analyse am Buildserver}
		\label{fig:jenkinsBuildResult}}
	{\includegraphics[scale=0.3]{\figdir/jenkinsResult}}\\~\\
				\tiny{\quelle\url{https://gatling.io/wp-content/uploads/2016/12/jenkins2.png}}
\end{figure}
Abbildung \ref{fig:jenkinsBuildResult} zeigt einen Graphen, der auf dem Buildserver \glqq Jenkins\grqq{} während eines Buildvorgangs generiert wurde. Die Grafik zeigt den \glqq Performance Trend\grqq{} einer Anwendung über mehrere Analysen hinweg. Wie aus dem Graph entnommen werden kann, gab es zwischen dem achten und zehnten Buildvorgang stetige Performance Probleme, die aber dann im elften Buildvorgang gelöst werden konnten. So wurden potentielle Bottlenecks, die unabsichtlich durch den Entwickler eingebaut wurden entdeckt und konnten nach dem Buildvorgang behoben werden. Die Anwendung musste nicht erst produktiv gehen, damit diese Probleme erkannt wurden.

Wie kann Gatling in den bestehenden / zukünftigen Continous Integration Prozess eingebunden werden?

Beispielhaft Jenkins auf Hersteller-seite vorgestellt.


\section{Tests}
In diesem Unterkapitel soll auf den grundsätzlichen Aufbau eines Gatling Tests eingangen werden.\\
Gatling Tests sind in der Programmiersprache Scala\footnote{{} Scala ist eine Java-ähnliche Sprache, die auf der \ac{JVM} läuft.} implementiert.
Im Code-Beispiel \ref{testCodingSample} ist beispielhaft ein solcher Test dargestellt. Dieser soll die Schnittstelle für das zufällige Abholen eines Witzes auf Last testen. Die grundsätzliche Funktionsweise der Schnittstelle wird an dieser Stelle vorausgesetzt. Dies muss im Vornherein durch Unit - bzw. Integrationstests sichergestellt werden. Für das Testen der Auslastung müssen allerdings einige Details über die Schnittstelle bekannt sein. So muss u.a. der Server mit zugehörigem Port, sowie der genaue Aufbau der Schnittstelle für den Entwickler einsehbar sein. Dieser Aspekt wird an dieser Stelle erwähnt, da es in größeren Unternehmen durchaus sein könnte, dass unabhängige Teams die gleiche Anwendung testen müssen. In diesem Fall hätte der Entwickler des Performance-Tests, nur die Black-Box-Sicht auf die bereits programmierte Anwendung. Das Code Beispiel \ref{testCodingSample} soll nun im Detail erläutert werden.\\
Zuerst werden die notwendigen Bibliotheken über \glqq Import-Befehle\grqq{} geladen. Die Klasse \glqq RandomJokeSimulation\grqq{} erbt von der definierten Klasse \glqq Simulation\grqq{}.\footnote{{} Jeder Gatling.io Test muss von der Simulation Klasse erben.} Anschließend wird die Verbindung konfiguriert, sowie der notwendige Test-Request zusammengebaut. In diesem Beispiel läuft die Anwendung im internen Netz an der Adresse \glqq http://192.168.111.20\grqq{} auf Port 58080. Damit der Request vom Server akzeptiert wird, werden die notwendigen Header gesetzt. \\
Anschließend wird die zu testende Logik, in Gatling.io \glqq Scenario\grqq{} genannt, implementiert. Das Scenario bekommt einen Namen zugewiesen und die Aufforderung, wie oft dieses ausgeführt werden soll. Zudem wird der genaue Pfad der Schnittstelle definiert und die \ac{HTTP} Methode, die auf die \ac{REST} Schnittstelle ausgeführt werden soll, ausgewählt.\\
Der Lasttest soll möglichst unter den Bedingungen durchgeführt werden, die auch im produktiven System aktiv sind. So ist es unrealistisch anzunehmen dass 1000 Benutzer exakt zeitgleich die Abfrage auf die Schnittstelle absetzen. Es ist vielmehr anzunehmen, dass die Benutzeranzahl mit zunehmender Laufzeit des Programms ansteigen wird. Genau diese Funktionalität bietet Gatling.io an. Zur Test Laufzeit können kontinuierlich, dies ist konfigurierbar, neue Benutzer erstellt werden, die auch Abfragen auf die Schnittstelle absetzen. So kann ein realistischer Use-Case getestet werden.\\ 

\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single,caption=Testabfrage auf Schnittstelle, label=testCodingSample, language=Scala]
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration._

class RandomJokeSimulation extends Simulation {
    val httpConf = http
        .baseURL("http://192.168.111.20:58080")
        .acceptHeader("application/json")
        .acceptEncodingHeader("gzip, deflate")
        .userAgentHeader("Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0")

    val scn = scenario("RandomJokeSimulation").repeat(100) {
        exec(http("getRandomJoke")
        .get("/api/v1/joke/random"))
    }    

    setUp(
        scn.inject(atOnceUsers(100))
    ).protocols(httpConf)
}
\end{lstlisting}
\end{minipage}



Unterschiede Lasttests zu Unit / Integrationtests usw.
%https://jaxenter.de/mit-dem-testen-von-anwendungen-ist-es-so-eine-last-erst-recht-mit-lasttests-27564



Allgemein Alternative mit JMeter
%https://www.heise.de/developer/artikel/Last-und-Performance-Tests-mit-JMeter-oder-Gatling-3648505.html

JMeter vs Gatling
%https://octoperf.com/blog/2015/06/08/jmeter-vs-gatling/


\section{Zwischenfazit Lasttests}

Performance oder Lasttests sind notwendig um eine stabile Anwendung bzw. API zu erreichen.
Langfristig wird sich die Architektur bzw. der Programmierprozess verbessern wenn die Ergebnisse der Tests analysiert, ausgewertet und auch für zukünftige Entwicklung berücksichtigt werden.


